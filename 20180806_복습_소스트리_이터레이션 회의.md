# 20180806 복습, 소스트리, 이터레이션 회의

## 1. 국민청원 데이터로 청원 카테고리 분류하기 (복습)

> 기존 분류에 기타 카테고리를 다른 카테고리로 예측하는 코드 추가

>1. **학습세트와 테스트세트를 7:3으로 임의로 나누기**
>
>2. **데이터 전처리**
>
>3. **단어 벡터화(BOW, TF-IDF)**
>
>   a.**기타 청원과 아닌 것 나누기**
>
>   b.**기타 청원 분류하기**
>
>   c.**예측값을 category_pred 컬럼에 넣어주기**
>
>4. **분류기 설정하기(RF, XGB)**
>
>5. **분류기로 학습시키기(RF, XGB)**
>
>   a.**label 데이터를 예측한 category_pred 를 사용하기**
>
>6. **학습의 정확도 보기(RF, XGB)**
>
>7. **테스트 데이터 예측하기(RF, XGB)**
>
>8. **실제 데이터와 예측결과의 차이를 보기(RF, XGB)** 



#### 1) XGBoost로 예측 추가

```
# 부스팅 알고리즘은?
- 부스팅 알고리즘은 약한 예측모형들을 결합하여 강한 예측모형을 만드는 알고리즘
- 배깅과 유사하게 초기 샘플데이터로 다수의 분류기를 만들지만 배깅과 다르게 순차적이다.
- 랜덤포레스트의 배깅과는 다르게 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만듦
- 결정트리(Decision Tree) 알고리즘의 연장선에 있음
- 여러 개의 결정트리를 묶어 강력한 모델을 만드는 앙상블 방법
- 분류와 회귀에 사용할 수 있음
- 무작위성이 없으며 강력한 사전 가지치기를 사용
```



#### 2) 글꼴

```
# 나눔글꼴 설치
!apt install fonts-nanum

# Plotnine 패키지 설치
!pip install plotnine

# 기본 글꼴 변경
import matplotlib as mpl
mpl.font_manager._rebuild()
mpl.pyplot.rc('font', family='NanumBarunGothic')     # 나눔고딕이 없어서 Nanumsquare 사용했음

# 레티나 디스플레이 지원
%config InlineBackend.figure_format = 'retina'
```



#### 3) 데이터 로드

```
# 판다스를 통해 읽어오기
데이터명(원하는거) = pd.read_csv('주소~~~~~.csv', parse_dates=['start', 'end'])

# 데이터 크기 확인
데이터명.shape

# 기술통계
데이터명.describe()
```

```
# 전체 데이터 중 투표가 1000건 이상인 데이터를 가져온다. 
df = 데이터명.loc[(petitions['votes'] > 1000)].copy()     # copy() 데이터복사
```
```
# 카테고리별 데이터 수
df['category'].value_counts()
```



#### 4) 파이썬 pandas DataFrame의 iloc, loc, ix의 차이!

##### (1) .iloc

- integer positon를 통해 값을 찾을 수 있다. label로는 찾을 수 없다 

##### (2) .loc

- label 을 통해 값을 찾을 수 있다. integer position로는 찾을 수 없다. 

##### (3) .ix

- integer position과 label모두 사용 할 수 있다. 만약 label이 숫자라면 label-based index만 된다.



#### 5) 막대그래프

```
%matplotlib inline 
import matplotlib.pyplot as plt

plt.rcParams["font.family"] = 'NanumBarunGothic'    
category_count.plot(kind='bar')     # 'kind=bar'는 plot종류중에서 막대그래프

(jupyter nootbook은 명령프롬프트에서 'pip install matplotlib'을 실행해준다.) 
```



#### 6) 전처리 하기

```
def preprocessing(text):
    # 개행문자 제거
    text= str(text)
    text = re.sub('\\\\n', ' ', text)
    # 특수문자 제거
    # 특수문자나 이모티콘 등은 때로는 의미를 갖기도 하지만 여기에서는 제거했습니다.
    # text = re.sub('[?.,;:|\)*~`’!^\-_+<>@\#$%&-=#}※]', '', text)
    # 한글, 영문, 숫자만 남기고 모두 제거하도록 합니다.
    # text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]', ' ', text)
    # 한글, 영문만 남기고 모두 제거하도록 합니다.
    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]', ' ', text)
    return text
```

```
# 불용어 제거
def remove_stopwords(text):
    tokens = text.split(' ')
    stops = ['수', '그', '있는', '있습니다','년도', '합니다', '하는', '및', '제', '할', '하고', '더', '대한', '한', '그리고', '월', '저는', '없는', '입니다', '등', '일', '많은', '이런', '것은', '왜','같은', '같습니다', '없습니다', '위해', '한다']
    meaningful_words = [w for w in tokens if not w in stops]
    return ' '.join(meaningful_words)
```

```
# 걸리는 시간
%time 
```



#### 7) 파이썬 pandas의 map함수, apply함수, applymap함수 차이점!

(참고 : http://taewan.kim/post/numpy_sum_axis/)

|       | team        | againt      | fifa_rank |
| ----- | ----------- | ----------- | --------- |
| **0** | russia      | saudiarabia | 65        |
| **1** | saudiarabia | russia      | 63        |
| **2** | egypt       | uruguay     | 31        |
| **3** | uruguay     | egypt       | 21        |

##### (1) map함수

- DataFrame 타입이 아니라, 반드시 Series 타입에서만 사용해야 한다. 

- Series는 NumPy에서 제공하는 1차원 배열과 비슷하지만 각 데이터의 의미를 표시하는 인덱스(index)를 붙일 수 있다. 하지만 데이터 자체는 그냥 값(value)의 1차원 배열이다. `map`함수는 Series의 이러한 값 하나하나에 접근하면서 해당 함수를 수행한다. 

- 보통 lamda함수와 자주 사용

  ```
  값(value) + 인덱스(index) = 시리즈 클래스(Series)
  ```

  ```
  df['winning_rate']  = df['team'].map(lambda x : total_record(x)[3])
  ```

  

##### (2) apply함수

-  [Series 객체](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html)에서도 사용가능하고, [DataFrame 객체](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html)에서도 사용가능하다.

- DataFrame에 복수 개의 컬럼이 필요할 때 사용한다.

  ```
  df['winning_rate'] = df.apply(lambda x:relative_record(x['team'], x['against'])[3], axis=1) 
  ```

  ##### axis(다차원 배열의 축)

  ```
  벡터에 <그림 1>을 적용해 보면, 벡터는 x 축만을 갖는 자료형입니다. 1차원 배열에 해당하는 벡터의 각 요소(Element)는 그 자체가 Row입니다.
  2차원 배열 형태의 행렬(matrix)은 x축의 행과 y축의 컬럼을 갖습니다. 2차원 배열 행렬은 depth가 1이라고 생각할 수 있습니다.
  3차원 배열 형태의 Tensor는 행과 열을 갖고 각 컬럼은 벡터 형태를 갖습니다. 이러한 벡터를 Depth로 표현합니다.
  4차원 이상의 배열은 z축의 depth 요소가 스칼라가 아니라 벡터 이상의 자료형을 갖는 것을 의미합니다. 이러한 방식으로 데이터의 Dimension(차원)은 끝없이 확장될 수 있습니다.
  ```

  

![다차원 배열에서 axis(축)의 의미](https://taewanmerepo.github.io/2017/09/numpy_axis/axis.jpg) 

##### (3) applymap함수

- DataFrame클래스의 함수이긴 하나, 위의 `apply`함수처럼 각 row(axis=1)나 각 column(axis=0)별로 작동하는 함수가 아니라, 각 요소(element)별로 작동한다.



#### 8)  기계학습으로 기타 제거하기

- 기타로 분류된 청원중에 특정 카테고리로 분류해 주면 좀 더 정확도가 높아질 것이라는 가설을 세워봅니다.
- 따라서 기타로 분류된 청원을 다른 청원을 바탕으로 학습시켜 분류해 봅니다.

```
df['category_pred'] = df['category'].copy()
df_not_etc = df.loc[df['category'] != '기타'].copy()     # '기타'카테고리를 제외한 모든 것
df_etc = df.loc[df['category'] == '기타'].copy()         # '기타'카테고리만 
print(df_not_etc.shape)
print(df_etc.shape)
```



#### 9) 기타 청원 분류하기 벡터화

- TFIDF를 사용해 벡터화

```
from sklearn.feature_extraction.text import CountVectorizer
```

```
stops = ['and', 'article', 'html', '수', '현', '있는', '있습니다', '그', '년도', '합니다', '하는', '및', '제', '할', '하고', '더', '대한', '한', '그리고', '월', '저는', '없는', '입니다', '등', '일', '많은', '이런', '것은', '왜','같은', '같습니다', '없습니다', '위해', '한다']

vectorizer = CountVectorizer(
analyzer = 'word',                       # 캐릭터 단위로 벡터화 할 수도 있습니다.
tokenizer = None,                        # 토크나이저를 따로 지정해 줄 수도 있습니다.
preprocessor = None,                     # 전처리 도구
stop_words = stops,                      # 불용어 nltk등의 도구를 사용할 수도 있습니다.
min_df = 2,                              # 토큰이 나타날 최소 문서 개수로 오타나 자주 나오지 않는 특                                            수한 전문용어 제거에 좋다. 
ngram_range=(1, 3),                      # BOW의 단위를 1~3개로 지정합니다. (5나6으로 해보기)
max_features = 2000                      # 만들 피처의 수, 단어의 수가 된다. (20000까지 늘려보기)
 )
vectorizer
```



#### 10) 기타 청원 분류하기

```
from sklearn.ensemble import RandomForestClassifier

# 랜덤포레스트 분류기를 사용 
(랜덤 포레스트의 estimators는 R에서 set.seed와 같다. random값을 고정시켜주는 것.=> 파라미터의 어떤 부분때문에 개선이 되었는지 판단하기 위해서 / 파라미터값은 통제할 수 없다.)

forest = RandomForestClassifier(
    n_estimators = 100, n_jobs = -1, random_state=2018)
forest
```



#### 11) 기타 카테고리로 분류한 데이터 합쳐주기

```
# concat는 데이터를 합쳐준다. 기타가 중간중간에 섞여있는 것을 기타카테고리로 뽑아서 정리하면, 인덱스 중간중간이 비어있는데 그것을 합쳐준 것.
df2 = pd.concat([df_not_etc, df_etc]) 
```



#### 12) 학습세트와 테스트세트 만들기

- 학습세트와 테스트세트를 7:3의 비율로 나눠 줍니다.

```
# reindex()는 index를 재설정. 중간중간 비어있는 것을 다시 정렬.
df2 = df2.reindex()
```

```
split_count = int(df2.shape[0] * 0.7)
split_count
```



#### 13) 랜덤 포레스트로 학습시키기

- 공식문서 : <http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>

```
from sklearn.ensemble import RandomForestClassifier

# 랜덤포레스트 분류기를 사용
forest = RandomForestClassifier(
    n_estimators = 100, n_jobs = -1, random_state=2018)
forest
```

```
# 어떤 분야의 청원인지 예측할 것이기 때문에 category를 넣어줍니다.
%time forest = forest.fit(train_feature_tfidf, y_label)
```



#### 14) XGBoost로 학습시키기

```
!pip install xgboost
```

```
import xgboost as xgb

# XGBoost Parameters http://xgboost.readthedocs.io/en/latest/parameter.html
xgb = xgb.XGBClassifier(n_estimators=10,
                          max_depth=6,
                          learning_rate=1.0,
                          max_delta_step=1,
                          eta = 1,
                          nthread=-1,
                          seed=2018)
xgb
```

```
%time xgb.fit(train_feature_tfidf, y_label)
```



---



## 2. 소스트리

> **로컬 PC와 GitHub 동기화 하기**
>
> * **TWL **
> * **Tutorial**
> * **10minutes to pandas**
> * **github에 push 할 때는 꼭 본인의 Repo에서 git pull --rebase upstream master 로 최신상태로 유지하고 push 해야지 기존 history가 보존된다.**



----



## 3. 이터레이션 회의

### D조

- 지역별 교회 위치 데이터(x)
- 대표 프랜차이즈 위치 데이터(스타벅스, 올리브영)
- **1조원 매출 브랜드 올리브영, 스타벅스가 비슷한 위치에 있음 인접해 있는 위치를 보고 공통적인 특성을 알아보고 어떤 연관이 있는지 시각화 하는것이 이번주 목표, 서울에 있는 위치를 찍고 인접한 곳들을 봐서 그 지역의 특성을 보려고 함** 

